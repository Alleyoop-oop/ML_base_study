"""
Time:2020/8/4
Author:tzh666
Content:ç”¨Pytorchæ„å»ºç¥ç»ç½‘ç»œ--ç”¨nnæ¨¡å—æ„å»ºä¸ä¸Šæ–‡ä»¶ç›¸åŒçš„ç½‘ç»œ
"""
from torch import nn
import torch.nn.functional as F


# ä¸ºç½‘ç»œåˆ›å»ºç±»æ—¶ï¼Œå¿…é¡»ç»§æ‰¿ nn.Module
class Network(nn.Module):
    def __init__(self):
        super().__init__()

        # Inputs to hidden layer linear transformation
        # åˆ›å»ºä¸€ä¸ªçº¿æ€§è½¬æ¢æ¨¡å—ğ‘¥ğ–+ğ‘ï¼Œå…¶ä¸­æœ‰784ä¸ªè¾“å…¥å’Œ256ä¸ªè¾“å‡º
        self.hidden = nn.Linear(784, 256)
        # Output layer, 10 units - one for each digit
        self.output = nn.Linear(256, 10)

        # Define sigmoid activation and softmax output
        self.sigmoid = nn.Sigmoid()
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        # Pass the input tensor through each of our operations
        x = self.hidden(x)
        x = self.sigmoid(x)
        x = self.output(x)
        x = self.softmax(x)

        return x


# å¦ä¸€ç§æ–¹å¼ æ›´ç®€æ˜“
# class Network(nn.Module):
#     def __init__(self):
#         super().__init__()
#         # Inputs to hidden layer linear transformation
#         self.hidden = nn.Linear(784, 256)
#         # Output layer, 10 units - one for each digit
#         self.output = nn.Linear(256, 10)
#
#     def forward(self, x):
#         # Hidden layer with sigmoid activation
#         x = F.sigmoid(self.hidden(x))
#         # Output layer with softmax activation
#         x = F.softmax(self.output(x), dim=1)
#
#         return x


# åˆ›å»ºä¸€ä¸ªNetworkå¯¹è±¡
model = Network()

# print(model)
for param in model.named_parameters():
    print(param)











